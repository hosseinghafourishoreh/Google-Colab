# Cell 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Cell 2: Install necessary libraries
!pip install rasterio
!pip install earthpy
!pip install tensorflow tensorflow_addons tensorflow_datasets tensorflow_hub numpy matplotlib seaborn sklearn

# Cell 3: Import libraries
import pandas as pd
import numpy as np
import keras
from keras import Sequential
from keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten, Input, GlobalMaxPooling1D
from keras.callbacks import EarlyStopping
from keras import Model
import rasterio
import earthpy.plot as ep
from keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import train_test_split
import matplotlib
from matplotlib import pyplot as plt
from matplotlib.colors import from_levels_and_colors
import os
import seaborn as sns
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_hub as hub
from osgeo import gdal, ogr
import numpy as np
import skimage.exposure
import os
from glob import glob
import matplotlib.pyplot as plt
import rasterio as rio
from rasterio.plot import plotting_extent
import geopandas as gpd
import earthpy as et
import earthpy.spatial as es
import earthpy.plot as ep
from rasterio.plot import show
from rasterio.mask import mask
import sys
import os
import subprocess
import datetime
import platform
import datetime
import glob
from osgeo import gdal
import geopandas as gpd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, Input, Lambda
import tensorflow.keras.backend as K

# Cell 4: Set parameters
FEATURES = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']
LABEL = ['ID_CLASS']
SPLIT = ['sample']
labels = ['label']
CLASSES = [1,2,3,4,5]
N_CLASSES = 5
PALETTE = ['#FF0000','#d20cf5','#FFFF00','#000080','#228B22']
SAMPLE_PATH = '/content/drive/MyDrive/Colab Notebooks/Karaj_FirstPaper/CNN_KARAJ1000.csv'
IMAGE_PATH = '/content/drive/MyDrive/Colab Notebooks/Karaj_FirstPaper/DATASET20.tif'

# Cell 5: Load and visualize data
dataset = gdal.Open(r'/content/drive/MyDrive/Colab Notebooks/Karaj_FirstPaper/DATASET20.tif')
print(dataset.RasterCount)

band4 = dataset.GetRasterBand(3) # Red Channel
band6 = dataset.GetRasterBand(2) # Green Channel
band8 = dataset.GetRasterBand(1) # Blue Channel

# Read the bands as Numpy arrays
b4 = band4.ReadAsArray()
b6 = band6.ReadAsArray()
b8 = band8.ReadAsArray()

# Plot the arrays using imshow()
img = np.dstack((1,20,29))
f = plt.figure()
plt.imshow(img)
plt.colorbar()

# Cell 6: Load image
image = rasterio.open(IMAGE_PATH)
bandNum = image.count
height = image.height
width = image.width
crs = image.crs
transform = image.transform
shape = (height, width)
image_vis = []
plot_size = (9, 9)

# Cell 7: Load sample data
samples = pd.read_csv(SAMPLE_PATH)
sample = samples.sample(frac=1) # Shuffle data
sample

# Cell 8: Split data into training and testing sets
train = samples[samples['sample'] == 'train']
test = samples[samples['sample'] == 'test']

train_features = train[FEATURES]
train_label = train[LABEL]
test_features = test[FEATURES]
test_label = test[LABEL]

def reshape_input(array):
    shape = array.shape
    return array.reshape(shape[0], shape[1], 1)

train_input = reshape_input(train_features.to_numpy())
test_input = reshape_input(test_features.to_numpy())

train_output = to_categorical(train_label.to_numpy(), N_CLASSES + 1, dtype=int)
test_output = to_categorical(test_label.to_numpy(), N_CLASSES + 1, dtype=int)

print(f'train features: {train_input.shape}\nTest features: {test_input.shape}\nTrain label: {train_output.shape}\nTest label: {test_output.shape}')

# Cell 9: Normalize data
train_input = train_input / np.max(train_input)
test_input = test_input / np.max(test_input)

# Cell 10: Check for NaN and Inf values
print("NaN values in train_input:", np.isnan(train_input).sum())
print("NaN values in test_input:", np.isnan(test_input).sum())
print("Inf values in train_input:", np.isinf(train_input).sum())
print("Inf values in test_input:", np.isinf(test_input).sum())
print("Max value in train_input:", np.max(train_input))
print("Min value in train_input:", np.min(train_input))

# Cell 11: Repeat NaN check
print(f"NaN values in train_input: {np.isnan(train_input).sum()}")
print(f"NaN values in test_input: {np.isnan(test_input).sum()}")

# Cell 12: Define and compile the model
def print_shape(x):
    print(K.int_shape(x))
    return x

from keras.models import Sequential
from keras.layers import Input, Conv3D, MaxPooling3D, Dropout, GlobalMaxPooling3D, Dense, BatchNormalization
from keras.regularizers import l2

train_shape = train_input.shape
input_shape = (train_shape[1], train_shape[2])

neuron = 86
drop = 0.4
kernel = 4
pool = 4

model = Sequential([
    Input(input_shape),
    Conv1D(neuron * 1, kernel, activation='relu'),
    Conv1D(neuron * 1, kernel, activation='relu'),
    Dropout(drop),
    Conv1D(neuron * 2, kernel, activation='relu'),
    Conv1D(neuron * 2, kernel, activation='relu'),
    Dropout(drop),
    Conv1D(neuron * 4, kernel, activation='relu'),
    Conv1D(neuron * 4, kernel, activation='relu'),
    Dropout(drop),
    GlobalMaxPooling1D(),
    Dense(neuron * 4, activation='relu'),
    Dropout(drop),
    Dense(neuron * 2, activation='relu'),
    Dropout(drop),
    Dense(neuron * 1, activation='relu'),
    Dropout(drop),
    Dense(N_CLASSES + 1, activation='softmax')
])

model.summary()

# Cell 13: Check output shapes
print("Shape of train_output:", train_output.shape)
print("Shape of test_output:", test_output.shape)
print("Unique values in train_output:", np.unique(train_output))
print("Unique values in test_output:", np.unique(test_output))

# Cell 14: Train the model
model.compile(optimizer='Adam', loss='CategoricalCrossentropy', metrics=['accuracy'])

stop = EarlyStopping(monitor='val_loss', patience=16)
result = model.fit(x=train_input, y=train_output, validation_data=(test_input, test_output), batch_size=8, callbacks=[stop], epochs=45)

# Cell 15: Plot training history
history = pd.DataFrame(result.history)
plt.figure(figsize=(8, 6))
plt.plot(range(len(history['accuracy'].values.tolist())), history['accuracy'].values.tolist(), label='Train_Accuracy')
plt.plot(range(len(history['loss'].values.tolist())), history['loss'].values.tolist(), label='Train_Loss')
plt.plot(range(len(history['val_accuracy'].values.tolist())), history['val_accuracy'].values.tolist(), label='Test_Accuracy')
plt.plot(range(len(history['val_loss'].values.tolist())), history['val_loss'].values.tolist(), label='Test_Loss')
plt.xlabel('Epochs')
plt.ylabel('Value')
plt.legend()
plt.show()

# Cell 16: Evaluate the model
prediction = np.argmax(model.predict(test_input), 1).flatten()
label = np.argmax(test_output, 1).flatten()

cm = confusion_matrix(label, prediction, normalize='true')
cm = ConfusionMatrixDisplay(cm)
cm.plot()

print(classification_report(label, prediction))

# Cell 17: Predict and visualize on the full image
image_input = []
for x in range(20):
    image_input.append(image.read(x + 1))
image_input = reshape_input(np.stack(image_input).reshape(20, -1).T)

prediction = model.predict(image_input, batch_size=1024 * 20)
prediction = np.argmax(prediction, 1)
prediction = prediction.reshape(shape[0], shape[1])

cmap, norm = from_levels_and_colors(CLASSES, PALETTE, extend='max')
ep.plot_bands(prediction, cmap=cmap, norm=norm, figsize=plot_size)

# Cell 18: Save prediction to a new file
save_location = '/content/drive/MyDrive/Colab Notebooks/Cuprite_Nevada/'
name = 'MPM.tif'
location = save_location + name

new_dataset = rasterio.open(location, mode='w', driver='GTiff', height=prediction.shape[0], width=prediction.shape[1], count=1, dtype=str(prediction.dtype), crs=crs, transform=transform)
new_dataset.write(prediction, 1)
new_dataset.close()
